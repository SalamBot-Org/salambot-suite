/**
 * @file        Flow de génération de réponses pour SalamBot
 * @author      SalamBot Team (contact: info@salambot.ma)
 * @created     2025-05-26
 * @updated     2025-05-26
 * @project     SalamBot - AI CRM for Moroccan SMEs
 */

import { flow } from 'genkit';
import { ReplyFlowInput, ReplyFlowOutput, SupportedLanguage } from './types';
import { GeminiModel } from 'genkit-vertexai';
import { OpenAIModel } from 'genkit-openai';
import * as fs from 'fs';
import * as path from 'path';

// Chargement des prompts
const promptsDir = path.join(__dirname, 'prompts');
const frArPrompt = fs.existsSync(path.join(promptsDir, 'fr_ar.prompt.txt')) 
  ? fs.readFileSync(path.join(promptsDir, 'fr_ar.prompt.txt'), 'utf-8')
  : 'Réponds de manière professionnelle et concise à ce message. Utilise le tutoiement de façon neutre.';

const darijaPrompt = fs.existsSync(path.join(promptsDir, 'darija.prompt.txt'))
  ? fs.readFileSync(path.join(promptsDir, 'darija.prompt.txt'), 'utf-8')
  : 'Réponds en arabe classique (ASM) à ce message écrit en dialecte marocain (Darija).';

/**
 * Flow de génération de réponse adaptée à la langue détectée
 * - Utilise Gemini Pro pour le français et l'arabe classique
 * - Utilise Llama 4 fine-tuné pour le Darija (via OpenAI API)
 */
export const replyFlow = flow<ReplyFlowInput, ReplyFlowOutput>({
  name: 'reply-flow',
  description: 'Génère une réponse adaptée à la langue détectée',
  version: '0.1.0',
  run: async (input) => {
    const startTime = Date.now();
    let modelUsed = '';
    let reply = '';
    
    try {
      // Sélection du modèle et du prompt en fonction de la langue détectée
      if (input.lang === 'fr' || input.lang === 'ar') {
        // Utilisation de Gemini Pro pour le français et l'arabe classique
        const gemini = new GeminiModel({
          model: 'gemini-pro',
          maxOutputTokens: 1024
        });
        
        const prompt = `${frArPrompt}\n\nMessage: "${input.message}"\n\nRéponse:`;
        const llmResult = await gemini.generate(prompt);
        reply = llmResult.text.trim();
        modelUsed = 'gemini-pro';
      } else if (input.lang === 'darija') {
        // Utilisation de Llama 4 fine-tuné pour le Darija
        // Note: Dans un environnement réel, on utiliserait un modèle spécifiquement fine-tuné
        const llama = new OpenAIModel({
          model: 'gpt-4', // Remplacer par 'llama-4-darija' en production
          maxTokens: 1024
        });
        
        const prompt = `${darijaPrompt}\n\nMessage: "${input.message}"\n\nRéponse:`;
        const llmResult = await llama.generate(prompt);
        reply = llmResult.text.trim();
        modelUsed = 'llama-4'; // En réalité, c'est gpt-4 pour le moment
      } else {
        // Fallback en cas de langue non supportée
        const gemini = new GeminiModel({
          model: 'gemini-pro',
          maxOutputTokens: 1024
        });
        
        const prompt = `${frArPrompt}\n\nMessage: "${input.message}"\n\nRéponse:`;
        const llmResult = await gemini.generate(prompt);
        reply = llmResult.text.trim();
        modelUsed = 'gemini-pro (fallback)';
      }
      
      return {
        reply,
        lang: input.lang,
        latency: Date.now() - startTime,
        modelUsed
      };
    } catch (error) {
      console.error('Erreur génération réponse:', error);
      
      // Réponse de secours en cas d'erreur
      return {
        reply: input.lang === 'fr' 
          ? "Désolé, je rencontre des difficultés techniques. Pourriez-vous reformuler votre message?"
          : input.lang === 'ar'
            ? "عذرًا، أواجه صعوبات تقنية. هل يمكنك إعادة صياغة رسالتك؟"
            : "عذرًا، أواجه صعوبات تقنية. هل يمكنك إعادة صياغة رسالتك؟",
        lang: input.lang,
        latency: Date.now() - startTime,
        modelUsed: 'fallback'
      };
    }
  }
});

/**
 * Fonction d'aide pour exécuter le flow de génération de réponse
 */
export async function generateReply(message: string, lang: SupportedLanguage): Promise<ReplyFlowOutput> {
  return await replyFlow.run({ message, lang });
}
