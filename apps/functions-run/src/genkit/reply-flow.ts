/**
 * ğŸŒŸ SALAMBOT REPLY ENGINE ğŸŒŸ
 *
 * ğŸ“– Story: Orchestration IA pour rÃ©ponses contextuelles multilingues
 * ğŸ­ Characters: Input â†’ Analyse â†’ SÃ©lection ModÃ¨le â†’ GÃ©nÃ©ration â†’ Output
 * ğŸ¬ Plot: DÃ©tection langue â†’ Prompt adaptatif â†’ RÃ©ponse culturellement adaptÃ©e
 *
 * ğŸ¯ Superpowers:
 *   â€¢ ğŸ§  Multi-modÃ¨les (Gemini + OpenAI + Fallbacks)
 *   â€¢ ğŸ‡²ğŸ‡¦ SpÃ©cialisation Darija avec contexte culturel
 *   â€¢ âš¡ SÃ©lection intelligente selon complexitÃ©
 *   â€¢ ğŸ“Š MÃ©triques temps-rÃ©el et observabilitÃ©
 *
 * ğŸ‘¨â€ğŸ’» Crafted by: SalamBot AI Research Team <ai@salambot.ma>
 * ğŸ“… Genesis: 2025-06-02 | Last Evolution: 2025-06-02
 * ğŸ·ï¸  Version: 2.1.0-neural | License: Proprietary
 */

import { flow } from 'genkit';
import { ReplyFlowInput, ReplyFlowOutput, SupportedLanguage } from './types';
import { GeminiModel } from 'genkit-vertexai';
import { OpenAIModel } from 'genkit-openai';
import * as fs from 'fs';
import * as path from 'path';

// Chargement des prompts
const promptsDir = path.join(__dirname, 'prompts');
const frArPrompt = fs.existsSync(path.join(promptsDir, 'fr_ar.prompt.txt'))
  ? fs.readFileSync(path.join(promptsDir, 'fr_ar.prompt.txt'), 'utf-8')
  : 'RÃ©ponds de maniÃ¨re professionnelle et concise Ã  ce message. Utilise le tutoiement de faÃ§on neutre.';

const darijaPrompt = fs.existsSync(path.join(promptsDir, 'darija.prompt.txt'))
  ? fs.readFileSync(path.join(promptsDir, 'darija.prompt.txt'), 'utf-8')
  : 'RÃ©ponds en arabe classique (ASM) Ã  ce message Ã©crit en dialecte marocain (Darija).';

/**
 * Flow de gÃ©nÃ©ration de rÃ©ponse adaptÃ©e Ã  la langue dÃ©tectÃ©e
 * - Utilise Gemini Pro pour le franÃ§ais et l'arabe classique
 * - Utilise Llama 4 fine-tunÃ© pour le Darija (via OpenAI API)
 */
export const replyFlow = flow<ReplyFlowInput, ReplyFlowOutput>({
  name: 'reply-flow',
  description: 'GÃ©nÃ¨re une rÃ©ponse adaptÃ©e Ã  la langue dÃ©tectÃ©e',
  version: '0.1.0',
  run: async (input) => {
    const startTime = Date.now();
    let modelUsed = '';
    let reply = '';

    try {
      // SÃ©lection du modÃ¨le et du prompt en fonction de la langue dÃ©tectÃ©e
      if (input.lang === 'fr' || input.lang === 'ar') {
        // Utilisation de Gemini Pro pour le franÃ§ais et l'arabe classique
        const gemini = new GeminiModel({
          model: 'gemini-pro',
          maxOutputTokens: 1024,
        });

        const prompt = `${frArPrompt}\n\nMessage: "${input.message}"\n\nRÃ©ponse:`;
        const llmResult = await gemini.generate(prompt);
        reply = llmResult.text.trim();
        modelUsed = 'gemini-pro';
      } else if (input.lang === 'darija') {
        // Utilisation de Llama 4 fine-tunÃ© pour le Darija
        // Note: Dans un environnement rÃ©el, on utiliserait un modÃ¨le spÃ©cifiquement fine-tunÃ©
        const llama = new OpenAIModel({
          model: 'gpt-4', // Remplacer par 'llama-4-darija' en production
          maxTokens: 1024,
        });

        const prompt = `${darijaPrompt}\n\nMessage: "${input.message}"\n\nRÃ©ponse:`;
        const llmResult = await llama.generate(prompt);
        reply = llmResult.text.trim();
        modelUsed = 'llama-4'; // En rÃ©alitÃ©, c'est gpt-4 pour le moment
      } else {
        // Fallback en cas de langue non supportÃ©e
        const gemini = new GeminiModel({
          model: 'gemini-pro',
          maxOutputTokens: 1024,
        });

        const prompt = `${frArPrompt}\n\nMessage: "${input.message}"\n\nRÃ©ponse:`;
        const llmResult = await gemini.generate(prompt);
        reply = llmResult.text.trim();
        modelUsed = 'gemini-pro (fallback)';
      }

      return {
        reply,
        lang: input.lang,
        latency: Date.now() - startTime,
        modelUsed,
      };
    } catch (error) {
      console.error('Erreur gÃ©nÃ©ration rÃ©ponse:', error);

      // RÃ©ponse de secours en cas d'erreur
      return {
        reply:
          input.lang === 'fr'
            ? 'DÃ©solÃ©, je rencontre des difficultÃ©s techniques. Pourriez-vous reformuler votre message?'
            : input.lang === 'ar'
            ? 'Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø§Øª ØªÙ‚Ù†ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø±Ø³Ø§Ù„ØªÙƒØŸ'
            : 'Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø£ÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø§Øª ØªÙ‚Ù†ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø±Ø³Ø§Ù„ØªÙƒØŸ',
        lang: input.lang,
        latency: Date.now() - startTime,
        modelUsed: 'fallback',
      };
    }
  },
});

/**
 * Fonction d'aide pour exÃ©cuter le flow de gÃ©nÃ©ration de rÃ©ponse
 */
export async function generateReply(
  message: string,
  lang: SupportedLanguage
): Promise<ReplyFlowOutput> {
  return await replyFlow.run({ message, lang });
}
